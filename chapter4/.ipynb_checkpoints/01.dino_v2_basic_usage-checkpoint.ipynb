{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57190157-201d-47e0-a714-b7f9a918c2af",
   "metadata": {},
   "source": [
    "# Dino-v2 Documentation\n",
    "Dino-v2 is a vision transformer model developed by Facebook Research. It is a successor to the original DINO model, designed for self-supervised learning in vision tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7830c4d-ff8a-4c5c-ab69-56753f3fcd10",
   "metadata": {},
   "source": [
    "# Installation\n",
    "Dino-v2 requires the following dependencies:\n",
    "\n",
    "Python 3.x\n",
    "PyTorch\n",
    "torchvision\n",
    "faiss\n",
    "numpy\n",
    "Pillow (PIL)\n",
    "OpenCV (cv2)\n",
    "tqdm\n",
    "matplotlib\n",
    "\n",
    "You can install these dependencies using pip:\n",
    "\n",
    "```pip install torch torchvision faiss-cpu numpy Pillow opencv-python tqdm matplotlib```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9655b02f-39ed-4a47-af23-a791a95038de",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model Description\n",
    "Dino-v2 uses a vision transformer architecture, similar to the original DINO model. It utilizes a transformer encoder to process input images. Specifically, it uses the dinov2_vits14 variant, which consists of 14 transformer layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cef8ab7-53a4-49a5-b474-943c52e84ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/shravan/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DinoVisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 384, kernel_size=(14, 14), stride=(14, 14))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x NestedTensorBlock(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): MemEffAttention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "  (head): Identity()\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Usage\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import supervision as sv\n",
    "\n",
    "# Load Dino-v2 model\n",
    "dinov2_vits14 = torch.hub.load(\"facebookresearch/dinov2\", \"dinov2_vits14\")\n",
    "\n",
    "# Choose device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move model to device\n",
    "dinov2_vits14.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e70673-88a7-413f-bc1d-7c81a429b914",
   "metadata": {},
   "source": [
    "Basic Usage\n",
    "Dino-v2 can be used for various computer vision tasks, including feature extraction, image classification, and object detection. Here's an example of using Dino-v2 for feature extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5809ff9-82ad-4137-a4ed-71a427327b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an image\n",
    "image_path = \"night-1927265_1280.jpg\"\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Preprocess the image\n",
    "transform = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "input_image = transform(image).unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebf44601-c60d-46a1-a212-07ff7b188c9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1923e683-9b0e-43c8-ad96-7a10555991fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get features from Dino-v2\n",
    "with torch.no_grad():\n",
    "    features = dinov2_vits14(input_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28e2eafb-ba1a-430f-afa4-39af86a42fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 384])\n"
     ]
    }
   ],
   "source": [
    "# Do something with the features\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704c99d1-6488-4598-acfb-692a20db54c7",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "Original DINO paper: https://arxiv.org/abs/2104.14294\n",
    "\n",
    "Facebook Research GitHub repository:https://github.com/facebookresearch/dino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6bef49-34b4-44db-8652-b603b9045aea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf-dev",
   "language": "python",
   "name": "hf_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
